{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"YoloV2_ZeroCostDL4Mic_beta_v1.ipynb","provenance":[{"file_id":"1EpgWlJK6U_ZwlBGiomLfbxx9UUtRPBTy","timestamp":1592904104821},{"file_id":"1f5usS6p8Cu_efegMwcR3v68AVOXBSyIf","timestamp":1588870626184},{"file_id":"1fM7obTEQKnSgVZMDa1KjiBgiBar2b0t8","timestamp":1588693012611},{"file_id":"1owWtQQucUxUOZMaPh2x_mxe_qXKHCZhp","timestamp":1588074588514},{"file_id":"159ARwlQE7-zi0EHxunOF_YPFLt-ZVU5x","timestamp":1587562499898},{"file_id":"1W-7NHehG5MRFILvZZzhPWWnOdJMkadb2","timestamp":1586332290412},{"file_id":"1pUetEQICxYWkYVaQIgdRH1EZBTl7oc2A","timestamp":1586292199692},{"file_id":"1MD36ZkM6XR9EuV12zimJmfCjzyeYZFWq","timestamp":1586269469061},{"file_id":"16A2mbaHzlEElntS8qkFBOsBvZG-mUeY6","timestamp":1586253795726},{"file_id":"1gJlcjOiSxr2buDOxmcFbT_d-GqwLjXtK","timestamp":1583343225796},{"file_id":"10yGI51WzHfgWgZAyE-EbkZFEvIOd6CP6","timestamp":1583171396283}],"collapsed_sections":[],"toc_visible":true},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"V9zNGvape2-I","colab_type":"text"},"source":["# **Yolo_v2**\n","\n","<font size = 4> Yolo v2 is an object detection model which detects and classifies objects in images. This is based on the second version of the original Yolo implementation which was published by [Redmon and Farhadi](https://ieeexplore.ieee.org/document/8100173).\n","\n","---\n","\n","<font size = 4>*Disclaimer*:\n","\n","<font size = 4>This notebook is based on the following paper: **YOLO9000: Better, Faster, Stronger**, Proceedings of the IEEE conference on computer vision and pattern recognition, 7263-7271, 2017, Joseph Redmon and Ali Farhadi, [https://ieeexplore.ieee.org/document/8100173](https://ieeexplore.ieee.org/document/8100173)\n","\n","<font size = 4>The source code for this notebook is adapted for keras and can be found in: [https://github.com/experiencor/keras-yolo2](https://github.com/experiencor/keras-yolo2)\n","\n","<font size = 4>The dataset used currently can be downloaded from [here](https://public.roboflow.ai/object-detection/bccd)\n","\n","\n","<font size = 4>**Please also cite this original paper when using or developing this notebook.**"]},{"cell_type":"markdown","metadata":{"id":"jWAz2i7RdxUV","colab_type":"text"},"source":["# **How to use this notebook?**\n","\n","---\n","\n","<font size = 4>Video describing how to use ZeroCostDL4Mic notebooks are available on youtube:\n","  - [**Video 1**](https://www.youtube.com/watch?v=GzD2gamVNHI&feature=youtu.be): Full run through of the workflow to obtain the notebooks and the provided test datasets as well as a common use of the notebook\n","  - [**Video 2**](https://www.youtube.com/watch?v=PUuQfP5SsqM&feature=youtu.be): Detailed description of the different sections of the notebook\n","\n","\n","---\n","###**Structure of a notebook**\n","\n","<font size = 4>The notebook contains two types of cell:  \n","\n","<font size = 4>**Text cells** provide information and can be modified by douple-clicking the cell. You are currently reading the text cell. You can create a new text by clicking `+ Text`.\n","\n","<font size = 4>**Code cells** contain code and the code can be modfied by selecting the cell. To execute the cell, move your cursor on the `[ ]`-mark on the left side of the cell (play button appears). Click to execute the cell. After execution is done the animation of play button stops. You can create a new coding cell by clicking `+ Code`.\n","\n","---\n","###**Table of contents, Code snippets** and **Files**\n","\n","<font size = 4>On the top left side of the notebook you find three tabs which contain from top to bottom:\n","\n","<font size = 4>*Table of contents* = contains structure of the notebook. Click the content to move quickly between sections.\n","\n","<font size = 4>*Code snippets* = contain examples how to code certain tasks. You can ignore this when using this notebook.\n","\n","<font size = 4>*Files* = contain all available files. After mounting your google drive (see section 1.) you will find your files and folders here. \n","\n","<font size = 4>**Remember that all uploaded files are purged after changing the runtime.** All files saved in Google Drive will remain. You do not need to use the Mount Drive-button; your Google Drive is connected in section 1.2.\n","\n","<font size = 4>**Note:** The \"sample data\" in \"Files\" contains default files. Do not upload anything in here!\n","\n","---\n","###**Making changes to the notebook**\n","\n","<font size = 4>**You can make a copy** of the notebook and save it to your Google Drive. To do this click file -> save a copy in drive.\n","\n","<font size = 4>To **edit a cell**, double click on the text. This will show you either the source code (in code cells) or the source text (in text cells).\n","You can use the `#`-mark in code cells to comment out parts of the code. This allows you to keep the original code piece in the cell as a comment."]},{"cell_type":"markdown","metadata":{"id":"vNMDQHm0Ah-Z","colab_type":"text"},"source":["#**0. Before getting started**\n","---\n","<font size = 4> Preparing the dataset carefully is essential to make this Yolo_v2 notebook work. This model requires as input a set of images (currently .jpg) and as target a list of annotation files in Pascal VOC format. The annotation files should have the exact same name as the input files, except with an .xml instead of the .jpg extension. The annotation files contain the class labels and all bounding boxes for the objects for each image in your dataset. Most datasets will give the option of saving the annotations in this format or using software for hand-annotations will automatically save the annotations in this format. \n","\n","<font size=4> If you want to assemble your own dataset we recommend using the open source https://www.makesense.ai/ resource.\n","\n","<font size = 4>**We strongly recommend that you generate extra paired images. These images can be used to assess the quality of your trained model (Quality control dataset)**. The quality control assessment can be done directly in this notebook.\n","\n","<font size = 4> **Additionally, the corresponding input and output files need to have the same name**.\n","\n","<font size = 4> Please note that you currently can **only use .png or .jpg files!**\n","\n","\n","<font size = 4>Here's a common data structure that can work:\n","*   Experiment A\n","    - **Training dataset**\n","      - Input images (Training_source)\n","        - img_1.jpg, img_2.jpg, ...\n","      - High SNR images (Training_source_annotations)\n","        - img_1.xml, img_2.xml, ...\n","    - **Quality control dataset**\n","     - Input images\n","        - img_1.jpg, img_2.jpg\n","      - High SNR images\n","        - img_1.xml, img_2.xml\n","    - **Data to be predicted**\n","    - **Results**\n","\n","---\n","<font size = 4>**Important note**\n","\n","<font size = 4>- If you wish to **Train a network from scratch** using your own dataset (and we encourage everyone to do that), you will need to run **sections 1 - 4**, then use **section 5** to assess the quality of your model and **section 6** to run predictions using the model that you trained.\n","\n","<font size = 4>- If you wish to **Evaluate your model** using a model previously generated and saved on your Google Drive, you will only need to run **sections 1 and 2** to set up the notebook, then use **section 5** to assess the quality of your model.\n","\n","<font size = 4>- If you only wish to **run predictions** using a model previously generated and saved on your Google Drive, you will only need to run **sections 1 and 2** to set up the notebook, then use **section 6** to run the predictions on the desired model.\n","---"]},{"cell_type":"markdown","metadata":{"id":"DMNHVZfHmbKb","colab_type":"text"},"source":["# **1. Initialise the Colab session**\n","---\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BCPhV-pe-syw","colab_type":"text"},"source":["\n","## **1.1. Check for GPU access**\n","---\n","\n","By default, the session should be using Python 3 and GPU acceleration, but it is possible to ensure that these are set properly by doing the following:\n","\n","<font size = 4>Go to **Runtime -> Change the Runtime type**\n","\n","<font size = 4>**Runtime type: Python 3** *(Python 3 is programming language in which this program is written)*\n","\n","<font size = 4>**Accelator: GPU** *(Graphics processing unit)*\n"]},{"cell_type":"code","metadata":{"id":"VNZetvLiS1qV","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Run this cell to check if you have GPU access\n","%tensorflow_version 1.x\n","\n","import tensorflow as tf\n","if tf.test.gpu_device_name()=='':\n","  print('You do not have GPU access.') \n","  print('Did you change your runtime ?') \n","  print('If the runtime setting is correct then Google did not allocate a GPU for your session')\n","  print('Expect slow performance. To access GPU try reconnecting later')\n","\n","else:\n","  print('You have GPU access')\n","  !nvidia-smi\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UBrnApIUBgxv","colab_type":"text"},"source":["## **1.2. Mount your Google Drive**\n","---\n","<font size = 4> To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\n","\n","<font size = 4> Play the cell below to mount your Google Drive and follow the link. In the new browser window, select your drive and select 'Allow', copy the code, paste into the cell and press enter. This will give Colab access to the data on the drive. \n","\n","<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."]},{"cell_type":"code","metadata":{"id":"01Djr8v-5pPk","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Run this cell to connect your Google Drive to Colab\n","\n","#@markdown * Click on the URL. \n","\n","#@markdown * Sign in your Google Account. \n","\n","#@markdown * Copy the authorization code. \n","\n","#@markdown * Enter the authorization code. \n","\n","#@markdown * Click on \"Files\" site on the right. Refresh the site. Your Google Drive folder should now be available here as \"drive\". \n","\n","#mounts user's Google Drive to Google Colab.\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n4yWFoJNnoin","colab_type":"text"},"source":["# **2. Install Yolo_v2 and Dependencies**\n","---\n"]},{"cell_type":"code","metadata":{"id":"3u2mXn3XsWzd","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Install Network and Dependencies\n","%tensorflow_version 1.x\n","!pip install pascal-voc-writer\n","from pascal_voc_writer import Writer\n","from __future__ import division\n","from __future__ import print_function\n","from __future__ import absolute_import\n","import csv\n","import random\n","import pprint\n","import sys\n","import time\n","import numpy as np\n","from optparse import OptionParser\n","import pickle\n","import math\n","import cv2\n","import copy\n","import math\n","from matplotlib import pyplot as plt\n","import matplotlib.patches as patches\n","import tensorflow as tf\n","import pandas as pd\n","import os\n","import shutil\n","from skimage import io\n","from sklearn.metrics import average_precision_score\n","\n","from keras.models import Model\n","from keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout, Reshape, Activation, Conv2D, MaxPooling2D, BatchNormalization, Lambda\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.layers.merge import concatenate\n","from keras.applications.mobilenet import MobileNet\n","from keras.applications import InceptionV3\n","from keras.applications.vgg16 import VGG16\n","from keras.applications.resnet50 import ResNet50\n","\n","from keras import backend as K\n","from keras.optimizers import Adam, SGD, RMSprop\n","from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, TimeDistributed\n","from keras.engine.topology import get_source_inputs\n","from keras.utils import layer_utils\n","from keras.utils.data_utils import get_file\n","from keras.objectives import categorical_crossentropy\n","from keras.models import Model\n","from keras.utils import generic_utils\n","from keras.engine import Layer, InputSpec\n","from keras import initializers, regularizers\n","from keras.utils import Sequence\n","import xml.etree.ElementTree as ET\n","from collections import OrderedDict, Counter\n","import json\n","import imageio\n","import imgaug as ia\n","from imgaug import augmenters as iaa\n","import copy\n","import cv2\n","from tqdm import tqdm\n","from tempfile import mkstemp\n","from shutil import move, copymode\n","from os import fdopen, remove\n","ia.seed(1)\n","# imgaug uses matplotlib backend for displaying images\n","from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n","import re\n","import glob\n","\n","if os.path.exists('/content/gdrive/My Drive/keras-yolo2'):\n","  shutil.rmtree('/content/gdrive/My Drive/keras-yolo2')\n","\n","!git clone https://github.com/experiencor/keras-yolo2.git\n","shutil.move('/content/keras-yolo2','/content/gdrive/My Drive/keras-yolo2')\n","\n","os.chdir('/content/gdrive/My Drive/keras-yolo2')\n","from backend import BaseFeatureExtractor, FullYoloFeature\n","from preprocessing import parse_annotation, BatchGenerator\n","\n","print(\"Depencies installed and imported.\")\n","\n","def plt_rectangle(plt,label,x1,y1,x2,y2,fontsize=10):\n","    '''\n","    == Input ==\n","    \n","    plt   : matplotlib.pyplot object\n","    label : string containing the object class name\n","    x1    : top left corner x coordinate\n","    y1    : top left corner y coordinate\n","    x2    : bottom right corner x coordinate\n","    y2    : bottom right corner y coordinate\n","    '''\n","    linewidth = 1\n","    color = \"yellow\"\n","    plt.text(x1,y1,label,fontsize=fontsize,backgroundcolor=\"magenta\")\n","    plt.plot([x1,x1],[y1,y2], linewidth=linewidth,color=color)\n","    plt.plot([x2,x2],[y1,y2], linewidth=linewidth,color=color)\n","    plt.plot([x1,x2],[y1,y1], linewidth=linewidth,color=color)\n","    plt.plot([x1,x2],[y2,y2], linewidth=linewidth,color=color)\n","\n","def extract_single_xml_file(tree,object_count=True):\n","    Nobj = 0\n","    row  = OrderedDict()\n","    for elems in tree.iter():\n","\n","        if elems.tag == \"size\":\n","            for elem in elems:\n","                row[elem.tag] = int(elem.text)\n","        if elems.tag == \"object\":\n","            for elem in elems:\n","                if elem.tag == \"name\":\n","                    row[\"bbx_{}_{}\".format(Nobj,elem.tag)] = str(elem.text)              \n","                if elem.tag == \"bndbox\":\n","                    for k in elem:\n","                        row[\"bbx_{}_{}\".format(Nobj,k.tag)] = float(k.text)\n","                    Nobj += 1\n","    if object_count == True:\n","      row[\"Nobj\"] = Nobj\n","    return(row)\n","\n","def count_objects(tree):\n","  Nobj=0\n","  for elems in tree.iter():\n","    if elems.tag == \"object\":\n","      for elem in elems:\n","        if elem.tag == \"bndbox\":\n","          Nobj += 1\n","  return(Nobj)\n","\n","#DetectionMap\n","\n","def intersect_area(box_a, box_b):\n","    \"\"\"\n","    Compute the area of intersection between two rectangular bounding box\n","    Bounding boxes use corner notation : [x1, y1, x2, y2]\n","    Args:\n","      box_a: (np.array) bounding boxes, Shape: [A,4].\n","      box_b: (np.array) bounding boxes, Shape: [B,4].\n","    Return:\n","      np.array intersection area, Shape: [A,B].\n","    \"\"\"\n","    resized_A = box_a[:, np.newaxis, :]\n","    resized_B = box_b[np.newaxis, :, :]\n","    max_xy = np.minimum(resized_A[:, :, 2:], resized_B[:, :, 2:])\n","    min_xy = np.maximum(resized_A[:, :, :2], resized_B[:, :, :2])\n","\n","    diff_xy = (max_xy - min_xy)\n","    inter = np.clip(diff_xy, a_min=0, a_max=np.max(diff_xy))\n","    return inter[:, :, 0] * inter[:, :, 1]\n","\n","    \n","def jaccard(box_a, box_b):\n","    \"\"\"\n","    Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n","    is simply the intersection over union of two boxes.  Here we operate on\n","    ground truth boxes and default boxes.\n","    E.g.:\n","        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)\n","    Args:\n","        box_a: (np.array) Predicted bounding boxes,    Shape: [n_pred, 4]\n","        box_b: (np.array) Ground Truth bounding boxes, Shape: [n_gt, 4]\n","    Return:\n","        jaccard overlap: (np.array) Shape: [n_pred, n_gt]\n","    \"\"\"\n","    inter = intersect_area(box_a, box_b)\n","    area_a = ((box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1]))\n","    area_b = ((box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1]))\n","    area_a = area_a[:, np.newaxis]\n","    area_b = area_b[np.newaxis, :]\n","    union = area_a + area_b - inter\n","    return inter / union\n","\n","\n","\n","\"\"\"\n","    Simple accumulator class that keeps track of True positive, False positive and False negative\n","    to compute precision and recall of a certain class\n","\"\"\"\n","\n","\n","class APAccumulator:\n","    def __init__(self):\n","        self.TP, self.FP, self.FN = 0, 0, 0\n","\n","    def inc_good_prediction(self, value=1):\n","        self.TP += value\n","\n","    def inc_bad_prediction(self, value=1):\n","        self.FP += value\n","\n","    def inc_not_predicted(self, value=1):\n","        self.FN += value\n","\n","    @property\n","    def precision(self):\n","        total_predicted = self.TP + self.FP\n","        if total_predicted == 0:\n","            total_gt = self.TP + self.FN\n","            if total_gt == 0:\n","                return 1.\n","            else:\n","                return 0.\n","        return float(self.TP) / total_predicted\n","\n","    @property\n","    def recall(self):\n","        total_gt = self.TP + self.FN\n","        if total_gt == 0:\n","            return 1.\n","        return float(self.TP) / total_gt\n","\n","    def __str__(self):\n","        str = \"\"\n","        str += \"True positives : {}\\n\".format(self.TP)\n","        str += \"False positives : {}\\n\".format(self.FP)\n","        str += \"False Negatives : {}\\n\".format(self.FN)\n","        str += \"Precision : {}\\n\".format(self.precision)\n","        str += \"Recall : {}\\n\".format(self.recall)\n","        return str\n","\n","\n","DEBUG = False\n","\n","\n","class DetectionMAP:\n","    def __init__(self, n_class, pr_samples=11, overlap_threshold=0.8):\n","        \"\"\"\n","        Running computation of average precision of n_class in a bounding box + classification task\n","        :param n_class:             quantity of class\n","        :param pr_samples:          quantification of threshold for pr curve\n","        :param overlap_threshold:   minimum overlap threshold\n","        \"\"\"\n","        self.n_class = n_class\n","        self.overlap_threshold = overlap_threshold\n","        self.pr_scale = np.linspace(0, 1, pr_samples)\n","        self.total_accumulators = []\n","        self.reset_accumulators()\n","\n","    def reset_accumulators(self):\n","        \"\"\"\n","        Reset the accumulators state\n","        TODO this is hard to follow... should use a better data structure\n","        total_accumulators : list of list of accumulators at each pr_scale for each class\n","        :return:\n","        \"\"\"\n","        self.total_accumulators = []\n","        for i in range(len(self.pr_scale)):\n","            class_accumulators = []\n","            for j in range(self.n_class):\n","                class_accumulators.append(APAccumulator())\n","            self.total_accumulators.append(class_accumulators)\n","\n","    def evaluate(self, pred_bb, pred_classes, pred_conf, gt_bb, gt_classes):\n","        \"\"\"\n","        Update the accumulator for the running mAP evaluation.\n","        For exemple, this can be called for each images\n","        :param pred_bb: (np.array)      Predicted Bounding Boxes [x1, y1, x2, y2] :     Shape [n_pred, 4]\n","        :param pred_classes: (np.array) Predicted Classes :                             Shape [n_pred]\n","        :param pred_conf: (np.array)    Predicted Confidences [0.-1.] :                 Shape [n_pred]\n","        :param gt_bb: (np.array)        Ground Truth Bounding Boxes [x1, y1, x2, y2] :  Shape [n_gt, 4]\n","        :param gt_classes: (np.array)   Ground Truth Classes :                          Shape [n_gt]\n","        :return:\n","        \"\"\"\n","\n","        if pred_bb.ndim == 1:\n","            pred_bb = np.repeat(pred_bb[:, np.newaxis], 4, axis=1)\n","        IoUmask = None\n","        if len(pred_bb) > 0:\n","            IoUmask = self.compute_IoU_mask(pred_bb, gt_bb, self.overlap_threshold)\n","        for accumulators, r in zip(self.total_accumulators, self.pr_scale):\n","            if DEBUG:\n","                print(\"Evaluate pr_scale {}\".format(r))\n","            self.evaluate_(IoUmask, accumulators, pred_classes, pred_conf, gt_classes, r)\n","\n","    @staticmethod\n","    def evaluate_(IoUmask, accumulators, pred_classes, pred_conf, gt_classes, confidence_threshold):\n","        pred_classes = pred_classes.astype(np.int)\n","        gt_classes = gt_classes.astype(np.int)\n","\n","        for i, acc in enumerate(accumulators):\n","            gt_number = np.sum(gt_classes == i)\n","            pred_mask = np.logical_and(pred_classes == i, pred_conf >= confidence_threshold)\n","            pred_number = np.sum(pred_mask)\n","            if pred_number == 0:\n","                acc.inc_not_predicted(gt_number)\n","                continue\n","\n","            IoU1 = IoUmask[pred_mask, :]\n","            mask = IoU1[:, gt_classes == i]\n","\n","            tp = DetectionMAP.compute_true_positive(mask)\n","            fp = pred_number - tp\n","            fn = gt_number - tp\n","            acc.inc_good_prediction(tp)\n","            acc.inc_not_predicted(fn)\n","            acc.inc_bad_prediction(fp)\n","\n","    @staticmethod\n","    def compute_IoU_mask(prediction, gt, overlap_threshold):\n","        IoU = jaccard(prediction, gt)\n","        # for each prediction select gt with the largest IoU and ignore the others\n","        for i in range(len(prediction)):\n","            maxj = IoU[i, :].argmax()\n","            IoU[i, :maxj] = 0\n","            IoU[i, (maxj + 1):] = 0\n","        # make a mask of all \"matched\" predictions vs gt\n","        return IoU >= overlap_threshold\n","\n","    @staticmethod\n","    def compute_true_positive(mask):\n","        # sum all gt with prediction of its class\n","        return np.sum(mask.any(axis=0))\n","\n","    def compute_ap(self, precisions, recalls):\n","        \"\"\"\n","        Compute average precision of a particular classes (cls_idx)\n","        :param cls:\n","        :return:\n","        \"\"\"\n","        previous_recall = 0\n","        average_precision = 0\n","        for precision, recall in zip(precisions[::-1], recalls[::-1]):\n","            average_precision += precision * (recall - previous_recall)\n","            previous_recall = recall\n","        return average_precision\n","\n","    def compute_precision_recall_(self, class_index, interpolated=True):\n","        precisions = []\n","        recalls = []\n","        for acc in self.total_accumulators:\n","            precisions.append(acc[class_index].precision)\n","            recalls.append(acc[class_index].recall)\n","\n","        if interpolated:\n","            interpolated_precision = []\n","            for precision in precisions:\n","                last_max = 0\n","                if interpolated_precision:\n","                    last_max = max(interpolated_precision)\n","                interpolated_precision.append(max(precision, last_max))\n","            precisions = interpolated_precision\n","        return precisions, recalls\n","\n","    def plot_pr(self, ax, class_name, precisions, recalls, average_precision):\n","        ax.step(recalls, precisions, color='b', alpha=0.2,\n","                where='post')\n","        ax.fill_between(recalls, precisions, step='post', alpha=0.2,\n","                        color='b')\n","        ax.set_ylim([0.0, 1.05])\n","        ax.set_xlim([0.0, 1.0])\n","        ax.set_xlabel('Recall')\n","        ax.set_ylabel('Precision')\n","        ax.set_title('{0:} : AUC={1:0.2f}'.format(class_name, average_precision))\n","\n","    def plot(self, interpolated=True, class_names=None):\n","        \"\"\"\n","        Plot all pr-curves for each classes\n","        :param interpolated: will compute the interpolated curve\n","        :return:\n","        \"\"\"\n","        grid = int(math.ceil(math.sqrt(self.n_class)))\n","        fig, axes = plt.subplots(nrows=grid, ncols=grid)\n","        mean_average_precision = []\n","        # TODO: data structure not optimal for this operation...\n","        for i, ax in enumerate(axes.flat):\n","            if i > self.n_class - 1:\n","                break\n","            precisions, recalls = self.compute_precision_recall_(i, interpolated)\n","            average_precision = self.compute_ap(precisions, recalls)\n","            class_name = class_names[i] if class_names else \"Class {}\".format(i)\n","            self.plot_pr(ax, class_name, precisions, recalls, average_precision)\n","            mean_average_precision.append(average_precision)\n","\n","        plt.suptitle(\"Mean average precision : {:0.2f}\".format(sum(mean_average_precision)/len(mean_average_precision)))\n","        fig.tight_layout(pad=2.0)\n","\n","\n","def show_frame(pred_bb, pred_classes, pred_conf, gt_bb, gt_classes, class_dict, background=np.zeros((512, 512, 3)), show_confidence=True):\n","    \"\"\"\n","    Plot the boundingboxes\n","    :param pred_bb: (np.array)      Predicted Bounding Boxes [x1, y1, x2, y2] :     Shape [n_pred, 4]\n","    :param pred_classes: (np.array) Predicted Classes :                             Shape [n_pred]\n","    :param pred_conf: (np.array)    Predicted Confidences [0.-1.] :                 Shape [n_pred]\n","    :param gt_bb: (np.array)        Ground Truth Bounding Boxes [x1, y1, x2, y2] :  Shape [n_gt, 4]\n","    :param gt_classes: (np.array)   Ground Truth Classes :                          Shape [n_gt]\n","    :param class_dict: (dictionary) Key value pairs of classes, e.g. {0:'dog',1:'cat',2:'horse'}\n","    :return:\n","    \"\"\"\n","    n_pred = pred_bb.shape[0]\n","    n_gt = gt_bb.shape[0]\n","    n_class = int(np.max(np.append(pred_classes, gt_classes)) + 1)\n","    #print(n_class)\n","    if len(background.shape) < 3:\n","      h, w = background.shape\n","    else:\n","      h, w, c = background.shape\n","\n","    ax = plt.subplot(\"111\")\n","    ax.imshow(background)\n","    cmap = plt.cm.get_cmap('hsv')\n","\n","    confidence_alpha = pred_conf.copy()\n","    if not show_confidence:\n","        confidence_alpha.fill(1)\n","\n","    for i in range(n_pred):\n","        x1 = pred_bb[i, 0] * w\n","        y1 = pred_bb[i, 1] * h\n","        x2 = pred_bb[i, 2] * w\n","        y2 = pred_bb[i, 3] * h\n","        rect_w = x2 - x1\n","        rect_h = y2 - y1\n","        #print(x1, y1)\n","        ax.add_patch(patches.Rectangle((x1, y1), rect_w, rect_h,\n","                                       fill=False,\n","                                       edgecolor=cmap(float(pred_classes[i]) / n_class),\n","                                       linestyle='dashdot',\n","                                       alpha=confidence_alpha[i]))\n","\n","    for i in range(n_gt):\n","        x1 = gt_bb[i, 0]# * w\n","        y1 = gt_bb[i, 1]# * h\n","        x2 = gt_bb[i, 2]# * w\n","        y2 = gt_bb[i, 3]# * h\n","        rect_w = x2 - x1\n","        rect_h = y2 - y1\n","        ax.add_patch(patches.Rectangle((x1, y1), rect_w, rect_h,\n","                                       fill=False,\n","                                       edgecolor=cmap(float(gt_classes[i]) / n_class)))\n","\n","    legend_handles = []\n","\n","    for i in range(n_class):\n","        legend_handles.append(patches.Patch(color=cmap(float(i) / n_class), label=class_dict[i]))\n","    \n","    ax.legend(handles=legend_handles)\n","    plt.show()\n","\n","class BoundBox:\n","    def __init__(self, xmin, ymin, xmax, ymax, c = None, classes = None):\n","        self.xmin = xmin\n","        self.ymin = ymin\n","        self.xmax = xmax\n","        self.ymax = ymax\n","        \n","        self.c     = c\n","        self.classes = classes\n","\n","        self.label = -1\n","        self.score = -1\n","\n","    def get_label(self):\n","        if self.label == -1:\n","            self.label = np.argmax(self.classes)\n","        \n","        return self.label\n","    \n","    def get_score(self):\n","        if self.score == -1:\n","            self.score = self.classes[self.get_label()]\n","            \n","        return self.score\n","\n","class WeightReader:\n","    def __init__(self, weight_file):\n","        self.offset = 4\n","        self.all_weights = np.fromfile(weight_file, dtype='float32')\n","        \n","    def read_bytes(self, size):\n","        self.offset = self.offset + size\n","        return self.all_weights[self.offset-size:self.offset]\n","    \n","    def reset(self):\n","        self.offset = 4\n","\n","def bbox_iou(box1, box2):\n","    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n","    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])  \n","    \n","    intersect = intersect_w * intersect_h\n","\n","    w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n","    w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n","    \n","    union = w1*h1 + w2*h2 - intersect\n","    \n","    return float(intersect) / union\n","\n","def draw_boxes(image, boxes, labels):\n","    image_h, image_w, _ = image.shape\n","    #Changes in box color added by LvC\n","    # class_colours = []\n","    # for c in range(len(labels)):\n","    #     colour = np.random.randint(low=0,high=255,size=3).tolist()\n","    #     class_colours.append(tuple(colour))\n","    for box in boxes:\n","        xmin = int(box.xmin*image_w)\n","        ymin = int(box.ymin*image_h)\n","        xmax = int(box.xmax*image_w)\n","        ymax = int(box.ymax*image_h)\n","        if box.get_label() == 0:\n","          cv2.rectangle(image, (xmin,ymin), (xmax,ymax), (255,0,0), 3)\n","        elif box.get_label() == 1:\n","          cv2.rectangle(image, (xmin,ymin), (xmax,ymax), (0,255,0), 3)\n","        else:#WBC\n","          cv2.rectangle(image, (xmin,ymin), (xmax,ymax), (0,0,255), 3)\n","        #cv2.rectangle(image, (xmin,ymin), (xmax,ymax), class_colours[box.get_label()], 3)\n","        cv2.putText(image, \n","                    labels[box.get_label()] + ' ' + str(round(box.get_score(),3)), \n","                    (xmin, ymin - 13), \n","                    cv2.FONT_HERSHEY_SIMPLEX, \n","                    1e-3 * image_h, \n","                    (0,0,0), 2)\n","    #print(box.get_label())    \n","    return image          \n","\n","#Function added by LvC\n","def save_boxes(image, boxes, labels):\n","    image_h, image_w, _ = image.shape\n","    save_boxes =[]\n","    for box in boxes:\n","        xmin = box.xmin\n","        save_boxes.append(xmin)\n","        ymin = box.ymin\n","        save_boxes.append(ymin)\n","        xmax = box.xmax\n","        save_boxes.append(xmax)\n","        ymax = box.ymax\n","        save_boxes.append(ymax)\n","        score = box.get_score()\n","        save_boxes.append(score)\n","        label = box.get_label()\n","        save_boxes.append(label)\n","    if not os.path.exists('/content/mycsv.csv'):\n","      with open('/content/mycsv.csv', 'w', newline='') as csvfile:\n","        csvwriter = csv.writer(csvfile, delimiter=',')\n","        csvwriter.writerow(save_boxes)\n","    else:\n","      with open('/content/mycsv.csv', 'a+', newline='') as csvfile:\n","        csvwriter = csv.writer(csvfile)\n","        csvwriter.writerow(save_boxes)\n","\n","        \n","def decode_netout(netout, anchors, nb_class, obj_threshold=0.5, nms_threshold=0.5):\n","    grid_h, grid_w, nb_box = netout.shape[:3]\n","\n","    boxes = []\n","    \n","    # decode the output by the network\n","    netout[..., 4]  = _sigmoid(netout[..., 4])\n","    netout[..., 5:] = netout[..., 4][..., np.newaxis] * _softmax(netout[..., 5:])\n","    netout[..., 5:] *= netout[..., 5:] > obj_threshold\n","    \n","    for row in range(grid_h):\n","        for col in range(grid_w):\n","            for b in range(nb_box):\n","                # from 4th element onwards are confidence and class classes\n","                classes = netout[row,col,b,5:]\n","                \n","                if np.sum(classes) > 0:\n","                    # first 4 elements are x, y, w, and h\n","                    x, y, w, h = netout[row,col,b,:4]\n","\n","                    x = (col + _sigmoid(x)) / grid_w # center position, unit: image width\n","                    y = (row + _sigmoid(y)) / grid_h # center position, unit: image height\n","                    w = anchors[2 * b + 0] * np.exp(w) / grid_w # unit: image width\n","                    h = anchors[2 * b + 1] * np.exp(h) / grid_h # unit: image height\n","                    confidence = netout[row,col,b,4]\n","                    \n","                    box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, confidence, classes)\n","                    \n","                    boxes.append(box)\n","\n","    # suppress non-maximal boxes\n","    for c in range(nb_class):\n","        sorted_indices = list(reversed(np.argsort([box.classes[c] for box in boxes])))\n","\n","        for i in range(len(sorted_indices)):\n","            index_i = sorted_indices[i]\n","            \n","            if boxes[index_i].classes[c] == 0: \n","                continue\n","            else:\n","                for j in range(i+1, len(sorted_indices)):\n","                    index_j = sorted_indices[j]\n","                    \n","                    if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_threshold:\n","                        boxes[index_j].classes[c] = 0\n","                        \n","    # remove the boxes which are less likely than a obj_threshold\n","    boxes = [box for box in boxes if box.get_score() > obj_threshold]\n","    \n","    return boxes\n","\n","def replace(file_path, pattern, subst):\n","    #Create temp file\n","    fh, abs_path = mkstemp()\n","    with fdopen(fh,'w') as new_file:\n","        with open(file_path) as old_file:\n","            for line in old_file:\n","                new_file.write(line.replace(pattern, subst))\n","    #Copy the file permissions from the old file to the new file\n","    copymode(file_path, abs_path)\n","    #Remove original file\n","    remove(file_path)\n","    #Move new file\n","    move(abs_path, file_path)\n","\n","with open(\"/content/gdrive/My Drive/keras-yolo2/frontend.py\", \"r\") as check:\n","  lineReader = check.readlines()\n","  reduce_lr = False\n","  for line in lineReader:\n","    if \"reduce_lr\" in line:\n","      reduce_lr = True\n","      break\n","\n","if reduce_lr == False:\n","  #replace(\"/content/gdrive/My Drive/keras-yolo2/frontend.py\",\"period=1)\",\"period=1)\\n        csv_logger=CSVLogger('/content/training_evaluation.csv')\")\n","  replace(\"/content/gdrive/My Drive/keras-yolo2/frontend.py\",\"period=1)\",\"period=1)\\n        reduce_lr=ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1)\")\n","replace(\"/content/gdrive/My Drive/keras-yolo2/frontend.py\",\"import EarlyStopping\",\"import ReduceLROnPlateau, EarlyStopping\")\n","replace(\"/content/gdrive/My Drive/keras-yolo2/frontend.py\", \"[early_stop, checkpoint, tensorboard]\",\"[checkpoint, reduce_lr]\")\n","\n","from frontend import YOLO\n","\n","def train(config_path, model_path, percentage_validation):\n","    #config_path = args.conf\n","\n","    with open(config_path) as config_buffer:    \n","        config = json.loads(config_buffer.read())\n","\n","    ###############################\n","    #   Parse the annotations \n","    ###############################\n","\n","    # parse annotations of the training set\n","    train_imgs, train_labels = parse_annotation(config['train']['train_annot_folder'], \n","                                                config['train']['train_image_folder'], \n","                                                config['model']['labels'])\n","\n","    # parse annotations of the validation set, if any, otherwise split the training set\n","    if os.path.exists(config['valid']['valid_annot_folder']):\n","        valid_imgs, valid_labels = parse_annotation(config['valid']['valid_annot_folder'], \n","                                                    config['valid']['valid_image_folder'], \n","                                                    config['model']['labels'])\n","    else:\n","        train_valid_split = int((1-percentage_validation/100.)*len(train_imgs))\n","        np.random.shuffle(train_imgs)\n","\n","        valid_imgs = train_imgs[train_valid_split:]\n","        train_imgs = train_imgs[:train_valid_split]\n","\n","    if len(config['model']['labels']) > 0:\n","        overlap_labels = set(config['model']['labels']).intersection(set(train_labels.keys()))\n","\n","        print('Seen labels:\\t', train_labels)\n","        print('Given labels:\\t', config['model']['labels'])\n","        print('Overlap labels:\\t', overlap_labels)           \n","\n","        if len(overlap_labels) < len(config['model']['labels']):\n","            print('Some labels have no annotations! Please revise the list of labels in the config.json file!')\n","            return\n","    else:\n","        print('No labels are provided. Train on all seen labels.')\n","        config['model']['labels'] = train_labels.keys()\n","        \n","    ###############################\n","    #   Construct the model \n","    ###############################\n","\n","    yolo = YOLO(backend             = config['model']['backend'],\n","                input_size          = config['model']['input_size'], \n","                labels              = config['model']['labels'], \n","                max_box_per_image   = config['model']['max_box_per_image'],\n","                anchors             = config['model']['anchors'])\n","\n","    ###############################\n","    #   Load the pretrained weights (if any) \n","    ###############################    \n","\n","    if os.path.exists(config['train']['pretrained_weights']):\n","        print(\"Loading pre-trained weights in\", config['train']['pretrained_weights'])\n","        yolo.load_weights(config['train']['pretrained_weights'])\n","\n","    ###############################\n","    #   Start the training process \n","    ###############################\n","\n","    yolo.train(train_imgs         = train_imgs,\n","               valid_imgs         = valid_imgs,\n","               train_times        = config['train']['train_times'],\n","               valid_times        = config['valid']['valid_times'],\n","               nb_epochs          = config['train']['nb_epochs'], \n","               learning_rate      = config['train']['learning_rate'], \n","               batch_size         = config['train']['batch_size'],\n","               warmup_epochs      = config['train']['warmup_epochs'],\n","               object_scale       = config['train']['object_scale'],\n","               no_object_scale    = config['train']['no_object_scale'],\n","               coord_scale        = config['train']['coord_scale'],\n","               class_scale        = config['train']['class_scale'],\n","               saved_weights_name = config['train']['saved_weights_name'],\n","               debug              = config['train']['debug'])\n","\n","# The training evaluation.csv is saved (overwrites the Files if needed). \n","    lossDataCSVpath = os.path.join(model_path,'Quality Control/training_evaluation.csv')\n","    with open(lossDataCSVpath, 'w') as f:\n","      writer = csv.writer(f)\n","      writer.writerow(['loss','val_loss', 'learning rate'])\n","      for i in range(len(yolo.model.history.history['loss'])):\n","        writer.writerow([yolo.model.history.history['loss'][i], yolo.model.history.history['val_loss'][i], yolo.model.history.history['lr'][i]])\n","\n","    yolo.model.save(model_path+'/last_weights.h5')\n","\n","def predict(config, weights_path, image_path):\n","\n","    with open(config) as config_buffer:    \n","        config = json.load(config_buffer)\n","\n","    ###############################\n","    #   Make the model \n","    ###############################\n","\n","    yolo = YOLO(backend             = config['model']['backend'],\n","                input_size          = config['model']['input_size'], \n","                labels              = config['model']['labels'], \n","                max_box_per_image   = config['model']['max_box_per_image'],\n","                anchors             = config['model']['anchors'])\n","\n","    ###############################\n","    #   Load trained weights\n","    ###############################    \n","\n","    yolo.load_weights(weights_path)\n","\n","    ###############################\n","    #   Predict bounding boxes \n","    ###############################\n","\n","    if image_path[-4:] == '.mp4':\n","        video_out = image_path[:-4] + '_detected' + image_path[-4:]\n","        video_reader = cv2.VideoCapture(image_path)\n","\n","        nb_frames = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n","        frame_h = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","        frame_w = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n","\n","        video_writer = cv2.VideoWriter(video_out,\n","                               cv2.VideoWriter_fourcc(*'MPEG'), \n","                               50.0, \n","                               (frame_w, frame_h))\n","\n","        for i in tqdm(range(nb_frames)):\n","            _, image = video_reader.read()\n","            \n","            boxes = yolo.predict(image)\n","            image = draw_boxes(image, boxes, config['model']['labels'])\n","\n","            video_writer.write(np.uint8(image))\n","\n","        video_reader.release()\n","        video_writer.release()  \n","    else:\n","        image = cv2.imread(image_path)\n","        boxes = yolo.predict(image)\n","        image = draw_boxes(image, boxes, config['model']['labels'])\n","        save_boxes(image,boxes,config['model']['labels'])#added by LvC\n","        print(len(boxes), 'boxes are found')\n","        #print(image)\n","        cv2.imwrite(image_path[:-4] + '_detected' + image_path[-4:], image)\n","\n","\n","# function to convert BoundingBoxesOnImage object into DataFrame\n","def bbs_obj_to_df(bbs_object):\n","#     convert BoundingBoxesOnImage object into array\n","    bbs_array = bbs_object.to_xyxy_array()\n","#     convert array into a DataFrame ['xmin', 'ymin', 'xmax', 'ymax'] columns\n","    df_bbs = pd.DataFrame(bbs_array, columns=['xmin', 'ymin', 'xmax', 'ymax'])\n","    return df_bbs\n","\n","# Function that will extract column data for our CSV file\n","def xml_to_csv(path):\n","    xml_list = []\n","    for xml_file in glob.glob(path + '/*.xml'):\n","        tree = ET.parse(xml_file)\n","        root = tree.getroot()\n","        for member in root.findall('object'):\n","            value = (root.find('filename').text,\n","                     int(root.find('size')[0].text),\n","                     int(root.find('size')[1].text),\n","                     member[0].text,\n","                     int(member[4][0].text),\n","                     int(member[4][1].text),\n","                     int(member[4][2].text),\n","                     int(member[4][3].text)\n","                     )\n","            xml_list.append(value)\n","    column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n","    xml_df = pd.DataFrame(xml_list, columns=column_name)\n","    return xml_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fw0kkTU6CsU4","colab_type":"text"},"source":["# **3. Select your paths and parameters**\n","\n","---\n","\n","<font size = 4>The code below allows the user to enter the paths to where the training data is and to define the training parameters.\n","\n","<font size = 4>After playing the cell will display some quantitative metrics of your dataset, including a count of objects per image and the number of instances per class.\n"]},{"cell_type":"markdown","metadata":{"id":"CB6acvUFtWqd","colab_type":"text"},"source":["# **3.1. Parameters and paths**\n","---\n","\n","<font size = 4>**`Training_source:`, `Training_source_annotations`:** These are the paths to your folders containing the Training_source and the annotation data respectively. To find the paths of the folders containing the respective datasets, go to your Files on the left of the notebook, navigate to the folder containing your files and copy the path by right-clicking on the folder, **Copy path** and pasting it into the right box below.\n","\n","<font size = 4>**`model_name`:** Use only my_model -style, not my-model (Use \"_\" not \"-\"). Do not use spaces in the name. Avoid using the name of an existing model (saved in the same folder) as it will be overwritten.\n","\n","<font size = 4>**`model_path`**: Enter the path where your model will be saved once trained (for instance your result folder).\n","\n","<font size = 5>**Training Parameters**\n","\n","<font size = 4>**`number_of_epochs`:**Give estimates for training performance given a number of epochs and provide a default value. **Default value:10**\n","\n","<font size = 4>**Note that Yolov2 uses 3 Warm-up epochs which improves the model's performance. This means the network will train for number_of_epochs + 3 epochs.**\n","\n","<font size = 4>**`backend`:** There are different backends which are available to be trained for Yolo. These are usually slightly different model architectures, with pretrained weights. Take a look at the available backends and research which one will be best suited for your dataset.\n","\n","<font size = 5>**Advanced Parameters - experienced users only**\n","\n","<font size =4>**`batch_size:`** This parameter defines the number of patches seen in each training step. Reducing or increasing the **batch size** may slow or speed up your training, respectively, and can influence network performance. **Default value: 16**\n","\n","<font size=4>**`false_negative_penalty`**: Penalize wrong detection of 'no-object'. **Default:5.0**\n","\n","<font size=4>**`false_positive_penalty`**: Penalize wrong detection of 'object'. **Default:1.0**\n","\n","<font size=4>**`position_size_penalty`**: Penalize inaccurate positioning or size of bounding boxes. **Default:1.0**\n","\n","<font size=4>**`false_class_penalty`**: Penalize misclassification of object in bounding box. **Default:1.0**\n","\n","<font size = 4>**`percentage_validation`:**  Input the percentage of your training dataset you want to use to validate the network during training. **Currently automatically implemented** **Default value: 20** "]},{"cell_type":"code","metadata":{"id":"ewpNJ_I0Mv47","colab_type":"code","cellView":"form","colab":{}},"source":["class bcolors:\n","  WARNING = '\\033[31m'\n","\n","#@markdown ###Path to training images:\n","\n","Training_Source = \"\" #@param {type:\"string\"}\n","\n","# Ground truth images\n","Training_Source_annotations = \"\" #@param {type:\"string\"}\n","\n","# model name and path\n","#@markdown ###Name of the model and path to model folder:\n","model_name = \"\" #@param {type:\"string\"}\n","model_path = \"\" #@param {type:\"string\"}\n","\n","# backend\n","#@markdown ###Choose a backend\n","#os.chdir(model_path+'/keras-yolo2')\n","backend = \"Full Yolo\" #@param [\"Select Model\",\"Full Yolo\",\"Inception3\",\"SqueezeNet\",\"MobileNet\",\"Tiny Yolo\"]\n","os.chdir('/content/gdrive/My Drive/keras-yolo2')\n","if backend == \"Full Yolo\":\n","  if not os.path.exists('/content/gdrive/My Drive/keras-yolo2/full_yolo_backend.h5'):\n","    !wget https://github.com/rodrigo2019/keras_yolo2/releases/download/pre-trained-weights/full_yolo_backend.h5\n","elif backend == \"Inception3\":\n","  if not os.path.exists('/content/gdrive/My Drive/keras-yolo2/inception_backend.h5'):\n","    !wget https://github.com/rodrigo2019/keras_yolo2/releases/download/pre-trained-weights/inception_backend.h5\n","elif backend == \"MobileNet\":\n","  if not os.path.exists('/content/gdrive/My Drive/keras-yolo2/mobilenet_backend.h5'):\n","    !wget https://github.com/rodrigo2019/keras_yolo2/releases/download/pre-trained-weights/mobilenet_backend.h5\n","elif backend == \"SqueezeNet\":\n","  if not os.path.exists('/content/gdrive/My Drive/keras-yolo2/squeezenet_backend.h5'):\n","    !wget https://github.com/rodrigo2019/keras_yolo2/releases/download/pre-trained-weights/squeezenet_backend.h5\n","elif backend == \"Tiny Yolo\":\n","  if not os.path.exists('/content/gdrive/My Drive/keras-yolo2/tiny_yolo_backend.h5'):\n","    !wget https://github.com/rodrigo2019/keras_yolo2/releases/download/pre-trained-weights/tiny_yolo_backend.h5\n","\n","#os.chdir('/content/drive/My Drive/Zero-Cost Deep-Learning to Enhance Microscopy/Various dataset/Detection_Dataset_2/BCCD.v2.voc')\n","#if not os.path.exists(model_path+'/full_raccoon.h5'):\n"," # !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1NWbrpMGLc84ow-4gXn2mloFocFGU595s' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1NWbrpMGLc84ow-4gXn2mloFocFGU595s\" -O full_yolo_raccoon.h5 && rm -rf /tmp/cookies.txt\n","\n","full_model_path = os.path.join(model_path,model_name)\n","if os.path.exists(full_model_path):\n","  print('Existing model path will be overwritten')\n","  shutil.rmtree(full_model_path)\n","os.mkdir(full_model_path)\n","\n","full_model_file_path = full_model_path+'/best_weights.h5'\n","os.chdir('/content/gdrive/My Drive/keras-yolo2/')\n","\n","#Change backend name\n","!sed -i 's@\\\"backend\\\":.*,@\\\"backend\\\":              \\\"$backend\\\",@g' config.json\n","\n","#Change the name of the training folder\n","!sed -i 's@\\\"train_image_folder\\\":.*,@\\\"train_image_folder\\\":   \\\"$Training_Source/\\\",@g' config.json\n","\n","#Change annotation folder\n","!sed -i 's@\\\"train_annot_folder\\\":.*,@\\\"train_annot_folder\\\":   \\\"$Training_Source_annotations/\\\",@g' config.json\n","\n","#Change the name of the saved model\n","!sed -i 's@\\\"saved_weights_name\\\":.*,@\\\"saved_weights_name\\\":   \\\"$full_model_file_path\\\",@g' config.json\n","\n","#Change warmup epochs for untrained model\n","!sed -i 's@\\\"warmup_epochs\\\":.*,@\\\"warmup_epochs\\\":        3,@g' config.json\n","\n","#When defining a new model we should reset the pretrained model parameter\n","!sed -i 's@\\\"pretrained_weights\\\":.*,@\\\"pretrained_weights\\\":   \\\"No_pretrained_weights\\\",@g' config.json\n","\n","# other parameters for training.\n","#@markdown ###Training Parameters\n","#@markdown Number of epochs:\n","\n","number_of_epochs =  10#@param {type:\"number\"}\n","\n","!sed -i 's@\\\"nb_epochs\\\":.*,@\\\"nb_epochs\\\":            $number_of_epochs,@g' config.json\n","\n","#@markdown ###Advanced Parameters\n","\n","Use_Default_Advanced_Parameters = False #@param {type:\"boolean\"}\n","#@markdown ###If not, please input:\n","\n","batch_size =  16#@param {type:\"number\"}\n","learning_rate = 1e-4 #@param{type:\"number\"}\n","false_negative_penalty = 5.0 #@param{type:\"number\"}\n","false_positive_penalty = 1.0 #@param{type:\"number\"}\n","position_size_penalty = 1.0 #@param{type:\"number\"}\n","false_class_penalty = 1.0 #@param{type:\"number\"}\n","percentage_validation = 20 #@param{type:\"number\"}\n","\n","if (Use_Default_Advanced_Parameters): \n","  print(\"Default advanced parameters enabled\")\n","  batch_size = 8\n","  learning_rate = 1e-4\n","  false_negative_penalty = 5.0\n","  false_positive_penalty = 1.0\n","  position_size_penalty = 1.0\n","  false_class_penalty = 1.0\n","\n","!sed -i 's@\\\"batch_size\\\":.*,@\\\"batch_size\\\":           $batch_size,@g' config.json\n","\n","!sed -i 's@\\\"learning_rate\\\":.*,@\\\"learning_rate\\\":        $learning_rate,@g' config.json\n","\n","!sed -i 's@\\\"object_scale\":.*,@\\\"object_scale\\\":         $false_negative_penalty,@g' config.json\n","\n","!sed -i 's@\\\"no_object_scale\":.*,@\\\"no_object_scale\\\":      $false_positive_penalty,@g' config.json\n","\n","!sed -i 's@\\\"coord_scale\\\":.*,@\\\"coord_scale\\\":          $position_size_penalty,@g' config.json\n","\n","!sed -i 's@\\\"class_scale\\\":.*,@\\\"class_scale\\\":          $false_class_penalty,@g' config.json\n","\n","df_anno = []\n","dir_anno = Training_Source_annotations\n","for fnm in os.listdir(dir_anno):  \n","    if not fnm.startswith('.'): ## do not include hidden folders/files\n","        tree = ET.parse(os.path.join(dir_anno,fnm))\n","        row = extract_single_xml_file(tree)\n","        row[\"fileID\"] = os.path.splitext(fnm)[0]\n","        df_anno.append(row)\n","df_anno = pd.DataFrame(df_anno)\n","\n","maxNobj = np.max(df_anno[\"Nobj\"])\n","\n","#Write the annotations to a csv file\n","df_anno.to_csv(model_path+'/annot.csv', index=False)#header=False, sep=',')\n","\n","#Show how many objects there are in the images\n","plt.figure()\n","plt.subplot(2,1,1)\n","plt.hist(df_anno[\"Nobj\"].values,bins=50)\n","plt.title(\"max N of objects per image={}\".format(maxNobj))\n","plt.show()\n","\n","#Show the classes and how many there are of each in the dataset\n","from collections import Counter\n","class_obj = []\n","for ibbx in range(maxNobj):\n","    class_obj.extend(df_anno[\"bbx_{}_name\".format(ibbx)].values)\n","class_obj = np.array(class_obj)\n","\n","count             = Counter(class_obj[class_obj != 'nan'])\n","print(count)\n","class_nm          = list(count.keys())\n","class_labels = json.dumps(class_nm)\n","class_count       = list(count.values())\n","asort_class_count = np.argsort(class_count)\n","\n","class_nm          = np.array(class_nm)[asort_class_count]\n","class_count       = np.array(class_count)[asort_class_count]\n","\n","!sed -i 's@\\\"labels\\\":.*@\\\"labels\\\":               $class_labels@g' config.json\n","xs = range(len(class_count))\n","\n","plt.subplot(2,1,2)\n","plt.barh(xs,class_count)\n","plt.yticks(xs,class_nm)\n","plt.title(\"The number of objects per class: {} objects in total\".format(len(count)))\n","plt.show()\n","\n","\n","#Generate anchors for the bounding boxes\n","import subprocess as sp\n","os.chdir('/content/gdrive/My Drive/keras-yolo2')\n","output = sp.getoutput('python ./gen_anchors.py -c ./config.json')\n","\n","anchors_1 = output.find(\"[\")\n","anchors_2 = output.find(\"]\")\n","\n","config_anchors = output[anchors_1:anchors_2+1]\n","!sed -i 's@\\\"anchors\\\":.*,@\\\"anchors\\\":              $config_anchors,@g' config.json\n","#here we check that no model with the same name already exist, if so delete\n","#if os.path.exists(model_path+'/'+model_name):\n"," # shutil.rmtree(model_path+'/'+model_name)\n","\n","Use_pretrained_model = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","cellView":"form","id":"NXxj-Xi3Kang","colab":{}},"source":["#@markdown ###Play this cell to visualise some example images from your dataset to make sure annotations and images are properly matched.\n","import imageio\n","  \n","size = 3    \n","ind_random = np.random.randint(0,df_anno.shape[0],size=size)\n","img_dir=Training_Source\n","\n","file_suffix = os.path.splitext(os.listdir(Training_Source)[0])[1]\n","for irow in ind_random:\n","    row  = df_anno.iloc[irow,:]\n","    path = os.path.join(img_dir, row[\"fileID\"] + file_suffix)\n","    # read in image\n","    img  = imageio.imread(path)\n","\n","    plt.figure(figsize=(12,12))\n","    plt.imshow(img) # plot image\n","    plt.title(\"Nobj={}, height={}, width={}\".format(row[\"Nobj\"],row[\"height\"],row[\"width\"]))\n","    # for each object in the image, plot the bounding box\n","    for iplot in range(row[\"Nobj\"]):\n","        plt_rectangle(plt,\n","                      label = row[\"bbx_{}_name\".format(iplot)],\n","                      x1=row[\"bbx_{}_xmin\".format(iplot)],\n","                      y1=row[\"bbx_{}_ymin\".format(iplot)],\n","                      x2=row[\"bbx_{}_xmax\".format(iplot)],\n","                      y2=row[\"bbx_{}_ymax\".format(iplot)])\n","    plt.show() ## show the plot"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eik5zLKWpN_O","colab_type":"text"},"source":["##**3.2. Data Augmentation**\n","\n","---\n","\n","<font size = 4> Data augmentation can improve training progress by amplifying differences in the dataset. This can be useful if the available dataset is small since, in this case, it is possible that a network could quickly learn every example in the dataset (overfitting), without augmentation. Augmentation is not necessary for training and if the dataset is large the values can be set to 0."]},{"cell_type":"code","metadata":{"id":"RmTSfMO-pNMc","colab_type":"code","colab":{},"cellView":"form"},"source":["#@markdown ##**Augmentation Options**\n","\n","def image_aug(df, images_path, aug_images_path, image_prefix, augmentor):\n","    # create data frame which we're going to populate with augmented image info\n","    aug_bbs_xy = pd.DataFrame(columns=\n","                              ['filename','width','height','class', 'xmin', 'ymin', 'xmax', 'ymax']\n","                             )\n","    grouped = df.groupby('filename')\n","    \n","    for filename in df['filename'].unique():\n","    #   get separate data frame grouped by file name\n","        group_df = grouped.get_group(filename)\n","        group_df = group_df.reset_index()\n","        group_df = group_df.drop(['index'], axis=1)   \n","    #   read the image\n","        image = imageio.imread(images_path+filename)\n","    #   get bounding boxes coordinates and write into array        \n","        bb_array = group_df.drop(['filename', 'width', 'height', 'class'], axis=1).values\n","    #   pass the array of bounding boxes coordinates to the imgaug library\n","        bbs = BoundingBoxesOnImage.from_xyxy_array(bb_array, shape=image.shape)\n","    #   apply augmentation on image and on the bounding boxes\n","        image_aug, bbs_aug = augmentor(image=image, bounding_boxes=bbs)\n","    #   disregard bounding boxes which have fallen out of image pane    \n","        bbs_aug = bbs_aug.remove_out_of_image()\n","    #   clip bounding boxes which are partially outside of image pane\n","        bbs_aug = bbs_aug.clip_out_of_image()\n","        \n","    #   don't perform any actions with the image if there are no bounding boxes left in it    \n","        if re.findall('Image...', str(bbs_aug)) == ['Image([]']:\n","            pass\n","        \n","    #   otherwise continue\n","        else:\n","        #   write augmented image to a file\n","            imageio.imwrite(aug_images_path+image_prefix+filename, image_aug)  \n","        #   create a data frame with augmented values of image width and height\n","            info_df = group_df.drop(['xmin', 'ymin', 'xmax', 'ymax'], axis=1)    \n","            for index, _ in info_df.iterrows():\n","                info_df.at[index, 'width'] = image_aug.shape[1]\n","                info_df.at[index, 'height'] = image_aug.shape[0]\n","        #   rename filenames by adding the predifined prefix\n","            info_df['filename'] = info_df['filename'].apply(lambda x: image_prefix+x)\n","        #   create a data frame with augmented bounding boxes coordinates using the function we created earlier\n","            bbs_df = bbs_obj_to_df(bbs_aug)\n","        #   concat all new augmented info into new data frame\n","            aug_df = pd.concat([info_df, bbs_df], axis=1)\n","        #   append rows to aug_bbs_xy data frame\n","            aug_bbs_xy = pd.concat([aug_bbs_xy, aug_df])            \n","    \n","    # return dataframe with updated images and bounding boxes annotations \n","    aug_bbs_xy = aug_bbs_xy.reset_index()\n","    aug_bbs_xy = aug_bbs_xy.drop(['index'], axis=1)\n","    return aug_bbs_xy\n","\n","Use_Data_augmentation = True #@param {type:\"boolean\"}\n","Use_Default_Augmentation_Parameters = False #@param {type:\"boolean\"}\n","\n","multiply_dataset_by = 3 #@param [2,3]\n","#@markdown ###If you are not using the default settings, please provide the values below:\n","\n","#@markdown ###**Image shift, zoom, shear and flip (%)**\n","\n","#horizontal_shift =  50 #@param {type:\"slider\", min:0, max:100, step:1}\n","#vertical_shift =  25 #@param {type:\"slider\", min:0, max:100, step:1}\n","#zoom_range =  10 #@param {type:\"slider\", min:0, max:100, step:1}\n","horizontal_flip = True #@param {type:\"boolean\"}\n","vertical_flip = True #@param {type:\"boolean\"}\n","\n","#@markdown ###**Rotate image within angle range (degrees):**\n","rotate_images = True #@param {type:\"boolean\"}\n","\n","#{type:\"slider\", min:0, max:90, step:1}\n","\n","if (Use_Default_Augmentation_Parameters):\n","  #horizontal_shift =  10\n","  #vertical_shift =  10\n","  #zoom_range =  10\n","  horizontal_flip = True\n","  vertical_flip = True\n","  if rotate_images==True:\n","    rotation_range = 90\n","  else:\n","    rotation_range = 0\n","\n","if (Use_Data_augmentation):\n","  print('Data Augmentation enabled')\n","  # load images as NumPy arrays and append them to images list\n","  if os.path.exists(Training_Source+'/.ipynb_checkpoints'):\n","    shutil.rmtree(Training_Source+'/.ipynb_checkpoints')\n","  \n","  images = []\n","  for index, file in enumerate(glob.glob(Training_Source+'/*'+file_suffix)):\n","      images.append(imageio.imread(file))\n","      \n","  # how many images we have\n","  print('Augmenting {} images'.format(len(images)))\n","\n","  # apply xml_to_csv() function to convert all XML files in images/ folder into labels.csv\n","  labels_df = xml_to_csv(Training_Source_annotations)\n","  labels_df.to_csv(('/content/original_labels.csv'), index=None)\n","  #print('Successfully converted xml to csv.')\n","  #labels_df\n","\n","  # This setup of augmentation parameters will pick two of four given augmenters and apply them in random order\n","  if horizontal_flip==True:\n","    h=1\n","  else:\n","    h=0\n","  if vertical_flip==True:\n","    v=1\n","  else:\n","    v=0\n","  aug = iaa.SomeOf(2, [    \n","      #iaa.Affine(scale=(1-zoom_range/100., 1+zoom_range/100.)),\n","      iaa.Affine(rotate=rotation_range, fit_output=True),\n","      #iaa.Affine(translate_percent={\"x\": (-horizontal_shift/100., horizontal_shift/100.), \"y\": (-vertical_shift/100., vertical_shift/100.)}),\n","      iaa.Fliplr(v),\n","      iaa.Flipud(h)\n","      #iaa.Multiply((0.5, 1.5)),\n","      #iaa.GaussianBlur(sigma=(1.0, 3.0)),\n","      #iaa.AdditiveGaussianNoise(scale=(0.03*255, 0.05*255))\n","  ])\n","  aug_2 = iaa.Affine(rotate=rotation_range, fit_output=True)\n","  #aug_3 = iaa.Affine(rotate=(0,rotation_range))\n","  #augmented_training_source = os.path.dirname(Training_Source)+'/'+os.path.basename(Training_Source)+'_with_augmentation'\n","  #augmented_images_df = image_aug(labels_df, Training_Source+'/', Training_Source+'/aug/', 'aug_', aug)\n","\n","  #Here we create a folder that will hold the original image dataset and the augmented image dataset\n","  augmented_training_source = os.path.dirname(Training_Source)+'/'+os.path.basename(Training_Source)+'_augmentation'\n","  if os.path.exists(augmented_training_source):\n","    shutil.rmtree(augmented_training_source)\n","  os.mkdir(augmented_training_source)\n","\n","  #Here we create a folder that will hold the original image annotation dataset and the augmented image annotation dataset (the bounding boxes).\n","  augmented_training_source_annotation = os.path.dirname(Training_Source_annotations)+'/'+os.path.basename(Training_Source_annotations)+'_augmentation'\n","  if os.path.exists(augmented_training_source_annotation):\n","    shutil.rmtree(augmented_training_source_annotation)\n","  os.mkdir(augmented_training_source_annotation)\n","\n","  #Create the augmentation\n","  augmented_images_df = image_aug(labels_df, Training_Source+'/', augmented_training_source+'/', 'aug_', aug)\n","  \n","  # Concat resized_images_df and augmented_images_df together and save in a new all_labels.csv file\n","  all_labels_df = pd.concat([labels_df, augmented_images_df])\n","  all_labels_df.to_csv('/content/combined_labels.csv', index=False)\n","\n","  #Here we convert the new bounding boxes for the augmented images to PASCAL VOC .xml format\n","  def convert_to_xml(df,source,target_folder):\n","    grouped = df.groupby('filename')\n","    for file in os.listdir(source):\n","      group_df = grouped.get_group(file)\n","      group_df = group_df.reset_index()\n","      group_df = group_df.drop(['index'], axis=1)\n","      #group_df = group_df.dropna(axis=0)\n","      writer = Writer(source+'/'+file,group_df.iloc[1]['width'],group_df.iloc[1]['height'])\n","      for i, row in group_df.iterrows():\n","        #if not row['xmin'] == '' or row['ymin'] == '':\n","        writer.addObject(row['class'],round(row['xmin']),round(row['ymin']),round(row['xmax']),round(row['ymax']))\n","        writer.save(target_folder+'/'+os.path.splitext(file)[0]+'.xml')\n","\n","  convert_to_xml(all_labels_df,augmented_training_source,augmented_training_source_annotation)\n","  \n","  #Second round of augmentation\n","  if multiply_dataset_by > 2:\n","    aug_labels_df_2 = xml_to_csv(augmented_training_source_annotation)\n","    augmented_images_2_df = image_aug(aug_labels_df_2, augmented_training_source+'/', augmented_training_source+'/', 'aug_2_', aug_2)\n","    all_aug_labels_df = pd.concat([augmented_images_df, augmented_images_2_df])\n","  #all_labels_df.to_csv('/content/all_labels_aug.csv', index=False)\n","  \n","    for file in os.listdir(augmented_training_source_annotation):\n","      os.remove(os.path.join(augmented_training_source_annotation,file))\n","  \n","    convert_to_xml(all_aug_labels_df,augmented_training_source,augmented_training_source_annotation)\n","\n","  # if multiply_dataset_by > 3:\n","  #   aug_labels_df_3 = xml_to_csv(augmented_training_source_annotation)\n","  #   augmented_images_3_df = image_aug(aug_labels_df_3, augmented_training_source+'/', augmented_training_source+'/', 'aug_3_', aug_2)\n","  #   all_aug_labels_df = pd.concat([all_aug_labels_df, augmented_images_3_df])\n","\n","\n","    for file in os.listdir(augmented_training_source_annotation):\n","      os.remove(os.path.join(augmented_training_source_annotation,file))\n","  \n","    convert_to_xml(all_aug_labels_df,augmented_training_source,augmented_training_source_annotation)\n","  \n","  for file in os.listdir(Training_Source):\n","\n","    shutil.copyfile(Training_Source+'/'+file,augmented_training_source+'/'+file)\n","    shutil.copyfile(Training_Source_annotations+'/'+os.path.splitext(file)[0]+'.xml',augmented_training_source_annotation+'/'+os.path.splitext(file)[0]+'.xml')\n","  # display new dataframe\n","  #augmented_images_df\n","  \n","  os.chdir('/content/gdrive/My Drive/keras-yolo2')\n","  #Change the name of the training folder\n","  !sed -i 's@\\\"train_image_folder\\\":.*,@\\\"train_image_folder\\\":   \\\"$augmented_training_source/\\\",@g' config.json\n","\n","  #Change annotation folder\n","  !sed -i 's@\\\"train_annot_folder\\\":.*,@\\\"train_annot_folder\\\":   \\\"$augmented_training_source_annotation/\\\",@g' config.json\n","\n","\n","else:\n","  print('No augmentation will be used')\n","\n","\n","df_anno = []\n","dir_anno = augmented_training_source_annotation\n","for fnm in os.listdir(dir_anno):  \n","    if not fnm.startswith('.'): ## do not include hidden folders/files\n","        tree = ET.parse(os.path.join(dir_anno,fnm))\n","        row = extract_single_xml_file(tree)\n","        row[\"fileID\"] = os.path.splitext(fnm)[0]\n","        df_anno.append(row)\n","df_anno = pd.DataFrame(df_anno)\n","\n","maxNobj = np.max(df_anno[\"Nobj\"])\n","\n","#Write the annotations to a csv file\n","#df_anno.to_csv(model_path+'/annot.csv', index=False)#header=False, sep=',')\n","\n","#Show how many objects there are in the images\n","plt.figure()\n","plt.subplot(2,1,1)\n","plt.hist(df_anno[\"Nobj\"].values,bins=50)\n","plt.title(\"max N of objects per image={}\".format(maxNobj))\n","plt.show()\n","\n","#Show the classes and how many there are of each in the dataset\n","from collections import Counter\n","class_obj = []\n","for ibbx in range(maxNobj):\n","    class_obj.extend(df_anno[\"bbx_{}_name\".format(ibbx)].values)\n","class_obj = np.array(class_obj)\n","\n","count             = Counter(class_obj[class_obj != 'nan'])\n","print(count)\n","class_nm          = list(count.keys())\n","class_labels = json.dumps(class_nm)\n","class_count       = list(count.values())\n","asort_class_count = np.argsort(class_count)\n","\n","class_nm          = np.array(class_nm)[asort_class_count]\n","class_count       = np.array(class_count)[asort_class_count]\n","\n","xs = range(len(class_count))\n","\n","plt.subplot(2,1,2)\n","plt.barh(xs,class_count)\n","plt.yticks(xs,class_nm)\n","plt.title(\"The number of objects per class: {} objects in total\".format(len(count)))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tZvcYmxTdXQm","colab_type":"code","colab":{},"cellView":"form"},"source":["#@markdown ###Play this cell to visualise some example images from your **augmented** dataset to make sure annotations and images are properly matched.\n","df_anno_aug = []\n","dir_anno_aug = augmented_training_source_annotation\n","for fnm in os.listdir(dir_anno_aug):  \n","    if not fnm.startswith('.'): ## do not include hidden folders/files\n","        tree = ET.parse(os.path.join(dir_anno_aug,fnm))\n","        row = extract_single_xml_file(tree)\n","        row[\"fileID\"] = os.path.splitext(fnm)[0]\n","        df_anno_aug.append(row)\n","df_anno_aug = pd.DataFrame(df_anno_aug)\n","\n","size = 3    \n","ind_random = np.random.randint(0,df_anno_aug.shape[0],size=size)\n","img_dir=augmented_training_source\n","\n","file_suffix = os.path.splitext(os.listdir(augmented_training_source)[0])[1]\n","for irow in ind_random:\n","    row  = df_anno_aug.iloc[irow,:]\n","    path = os.path.join(img_dir, row[\"fileID\"] + file_suffix)\n","    # read in image\n","    img  = imageio.imread(path)\n","\n","    plt.figure(figsize=(12,12))\n","    plt.imshow(img) # plot image\n","    plt.title(\"Nobj={}, height={}, width={}\".format(row[\"Nobj\"],row[\"height\"],row[\"width\"]))\n","    # for each object in the image, plot the bounding box\n","    for iplot in range(row[\"Nobj\"]):\n","        plt_rectangle(plt,\n","                      label = row[\"bbx_{}_name\".format(iplot)],\n","                      x1=row[\"bbx_{}_xmin\".format(iplot)],\n","                      y1=row[\"bbx_{}_ymin\".format(iplot)],\n","                      x2=row[\"bbx_{}_xmax\".format(iplot)],\n","                      y2=row[\"bbx_{}_ymax\".format(iplot)])\n","    plt.show() ## show the plot"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rQndJj70FzfL","colab_type":"text"},"source":["# **4. Train the network**\n","---"]},{"cell_type":"code","metadata":{"id":"_cvRRrStGe3y","colab_type":"code","cellView":"form","colab":{}},"source":["# @markdown ##Loading weights from a pretrained network\n","\n","# Training_Source = \"\" #@param{type:\"string\"}\n","# Training_Source_annotation = \"\" #@param{type:\"string\"}\n","# Check if the right files exist\n","\n","Use_pretrained_model = False #@param {type:\"boolean\"}\n","\n","Weights_choice = \"best\" #@param [\"last\", \"best\"]\n","\n","pretrained_model_path = \"\" #@param{type:\"string\"}\n","h5_file_path = pretrained_model_path+'/'+Weights_choice+'_weights.h5'\n","\n","if not os.path.exists(h5_file_path):\n","  print('WARNING pretrained model does not exist')\n","  Use_pretrained_model = False\n","\n","os.chdir('/content/gdrive/My Drive/keras-yolo2')\n","!sed -i 's@\\\"pretrained_weights\\\":.*,@\\\"pretrained_weights\\\":   \\\"$h5_file_path\\\",@g' config.json\n","\n","if Use_pretrained_model == True:\n","  with open(os.path.join(pretrained_model_path, 'Quality Control', 'training_evaluation.csv'),'r') as csvfile:\n","    csvRead = pd.read_csv(csvfile, sep=',')\n","    if \"learning rate\" in csvRead.columns: #Here we check that the learning rate column exist (compatibility with model trained un ZeroCostDL4Mic bellow 1.4):\n","      print(\"pretrained network learning rate found\")\n","      #find the last learning rate\n","      lastLearningRate = csvRead[\"learning rate\"].iloc[-1]\n","      #Find the learning rate corresponding to the lowest validation loss\n","      min_val_loss = csvRead[csvRead['val_loss'] == min(csvRead['val_loss'])]\n","      #print(min_val_loss)\n","      bestLearningRate = min_val_loss['learning rate'].iloc[-1]\n","\n","      if Weights_choice == \"last\":\n","        print('Last learning rate: '+str(lastLearningRate))\n","        learning_rate = lastLearningRate\n","\n","      if Weights_choice == \"best\":\n","        print('Learning rate of best validation loss: '+str(bestLearningRate))\n","        learning_rate = bestLearningRate\n","\n","      if not \"learning rate\" in csvRead.columns: #if the column does not exist, then initial learning rate is used instead\n","        #bestLearningRate = learning_rate\n","        #lastLearningRate = learning_rate\n","        print(bcolors.WARNING+'WARNING: The learning rate cannot be identified from the pretrained network. Default learning rate of '+str(bestLearningRate)+' will be used instead' + W)\n","  \n","  !sed -i 's@\\\"warmup_epochs\\\":.*,@\\\"warmup_epochs\\\":        0,@g' config.json\n","  !sed -i 's@\\\"learning_rate\\\":.*,@\\\"learning_rate\\\":        $learning_rate,@g' config.json\n","\n","# with open(os.path.join(pretrained_model_path, 'Quality Control', 'lr.csv'),'r') as csvfile:\n","#         csvRead = pd.read_csv(csvfile, sep=',')\n","#         #print(csvRead)\n","    \n","#         if \"learning rate\" in csvRead.columns: #Here we check that the learning rate column exist (compatibility with model trained un ZeroCostDL4Mic bellow 1.4)\n","#           print(\"pretrained network learning rate found\")\n","#           #find the last learning rate\n","#           lastLearningRate = csvRead[\"learning rate\"].iloc[-1]\n","#           #Find the learning rate corresponding to the lowest validation loss\n","#           min_val_loss = csvRead[csvRead['val_loss'] == min(csvRead['val_loss'])]\n","#           #print(min_val_loss)\n","#           bestLearningRate = min_val_loss['learning rate'].iloc[-1]\n","\n","#           if Weights_choice == \"last\":\n","#             print('Last learning rate: '+str(lastLearningRate))\n","\n","#           if Weights_choice == \"best\":\n","#             print('Learning rate of best validation loss: '+str(bestLearningRate))\n","\n","#         if not \"learning rate\" in csvRead.columns: #if the column does not exist, then initial learning rate is used instead\n","#           bestLearningRate = initial_learning_rate\n","#           lastLearningRate = initial_learning_rate\n","#           print(bcolors.WARNING+'WARNING: The learning rate cannot be identified from the pretrained network. Default learning rate of '+str(bestLearningRate)+' will be used instead' + W)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wQPz0F6JlvJR","colab_type":"text"},"source":["## **4.1. Train the network**\n","---\n","<font size = 4>When playing the cell below you should see updates after each epoch (round). Network training can take some time.\n","\n","<font size = 4>* **CRITICAL NOTE:** Google Colab has a time limit for processing (to prevent using GPU power for datamining). Training time must be less than 12 hours! If training takes longer than 12 hours, please decrease the number of epochs or number of patches."]},{"cell_type":"code","metadata":{"id":"EZnoS3rb8BSR","colab_type":"code","cellView":"form","colab":{}},"source":["import time\n","import csv\n","#from frontend import YOLO\n","\n","if os.path.exists(full_model_path+\"/Quality Control\"):\n","  shutil.rmtree(full_model_path+\"/Quality Control\")\n","os.makedirs(full_model_path+\"/Quality Control\")\n","\n","start = time.time()\n","\n","#@markdown ##Start Training\n","\n","#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n","#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","# Start Training\n","\n","os.chdir('/content/gdrive/My Drive/keras-yolo2')\n","train('config.json', full_model_path, percentage_validation)\n","\n","#shutil.move('/content/training_evaluation.csv',model_path+'/Quality Control/training_evaluation.csv')\n","#!python ./train.py -c ./config.json\n","#!python /content/drive/My\\ Drive/Zero-Cost\\ Deep-Learning\\ to\\ Enhance\\ Microscopy/Various\\ dataset/Detection_Dataset_2/BCCD.v1.voc/keras_yolo2/train.py -c /content/drive/My\\ Drive/Zero-Cost\\ Deep-Learning\\ to\\ Enhance\\ Microscopy/Various\\ dataset/Detection_Dataset_2/BCCD.v1.voc/keras_yolo2/config.json\n","#Insert the code necessary to initiate training of your model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XQjQb_J_Qyku","colab_type":"text"},"source":["##**4.3. Download your model(s) from Google Drive**\n","\n","\n","---\n","<font size = 4>Once training is complete, the trained model is automatically saved on your Google Drive, in the **model_path** folder that was selected in Section 3. It is however wise to download the folder as all data can be erased at the next training if using the same folder."]},{"cell_type":"markdown","metadata":{"id":"2HbZd7rFqAad","colab_type":"text"},"source":["# **5. Evaluate your model**\n","---\n","\n","<font size = 4>This section allows the user to perform important quality checks on the validity and generalisability of the trained model. \n","\n","<font size = 4>**We highly recommend to perform quality control on all newly trained models.**\n","\n"]},{"cell_type":"code","metadata":{"id":"EdcnkCr9Nbl8","colab_type":"code","cellView":"form","colab":{}},"source":["# model name and path\n","#@markdown ###Do you want to assess the model you just trained ?\n","Use_the_current_trained_model = True #@param {type:\"boolean\"}\n","\n","#@markdown ###If not, please provide the name of the model folder:\n","\n","QC_model_folder = \"\" #@param {type:\"string\"}\n","\n","if (Use_the_current_trained_model): \n","  QC_model_folder = full_model_path\n","\n","#print(os.path.join(model_path, model_name))\n","\n","if os.path.exists(QC_model_folder):\n","  print(\"The \"+os.path.basename(QC_model_folder)+\" model will be evaluated\")\n","else:\n","  W  = '\\033[0m'  # white (normal)\n","  R  = '\\033[31m' # red\n","  print(R+'!! WARNING: The chosen model does not exist !!'+W)\n","  print('Please make sure you provide a valid model path before proceeding further.')\n","\n","\n","#@markdown ###Which backend is the model using?\n","backend = \"Full Yolo\" #@param [\"Select Model\",\"Full Yolo\",\"Inception3\",\"SqueezeNet\",\"MobileNet\",\"Tiny Yolo\"]\n","os.chdir('/content/gdrive/My Drive/keras-yolo2')\n","if backend == \"Full Yolo\":\n","  if not os.path.exists('/content/gdrive/My Drive/keras-yolo2/full_yolo_backend.h5'):\n","    !wget https://github.com/rodrigo2019/keras_yolo2/releases/download/pre-trained-weights/full_yolo_backend.h5\n","elif backend == \"Inception3\":\n","  if not os.path.exists('/content/gdrive/My Drive/keras-yolo2/inception_backend.h5'):\n","    !wget https://github.com/rodrigo2019/keras_yolo2/releases/download/pre-trained-weights/inception_backend.h5\n","elif backend == \"MobileNet\":\n","  if not os.path.exists('/content/gdrive/My Drive/keras-yolo2/mobilenet_backend.h5'):\n","    !wget https://github.com/rodrigo2019/keras_yolo2/releases/download/pre-trained-weights/mobilenet_backend.h5\n","elif backend == \"SqueezeNet\":\n","  if not os.path.exists('/content/gdrive/My Drive/keras-yolo2/squeezenet_backend.h5'):\n","    !wget https://github.com/rodrigo2019/keras_yolo2/releases/download/pre-trained-weights/squeezenet_backend.h5\n","elif backend == \"Tiny Yolo\":\n","  if not os.path.exists('/content/gdrive/My Drive/keras-yolo2/tiny_yolo_backend.h5'):\n","    !wget https://github.com/rodrigo2019/keras_yolo2/releases/download/pre-trained-weights/tiny_yolo_backend.h5\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yDY9dtzdUTLh","colab_type":"text"},"source":["## **5.1. Inspection of the loss function**\n","---\n","\n","<font size = 4>First, it is good practice to evaluate the training progress by comparing the training loss with the validation loss. The latter is a metric which shows how well the network performs on a subset of unseen data which is set aside from the training dataset. For more information on this, see for example [this review](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6381354/) by Nichols *et al.*\n","\n","<font size = 4>**Training loss** describes an error value after each epoch for the difference between the model's prediction and its ground-truth target.\n","\n","<font size = 4>**Validation loss** describes the same error value between the model's prediction on a validation image and compared to it's target.\n","\n","<font size = 4>During training both values should decrease before reaching a minimal value which does not decrease further even after more training. Comparing the development of the validation loss with the training loss can give insights into the model's performance.\n","\n","<font size = 4>Decreasing **Training loss** and **Validation loss** indicates that training is still necessary and increasing the `number_of_epochs` is recommended. Note that the curves can look flat towards the right side, just because of the y-axis scaling. The network has reached convergence once the curves flatten out. After this point no further training is required. If the **Validation loss** suddenly increases again an the **Training loss** simultaneously goes towards zero, it means that the network is overfitting to the training data. In other words the network is remembering the exact patterns from the training data and no longer generalizes well to unseen data. In this case the training dataset has to be increased."]},{"cell_type":"code","metadata":{"id":"vMzSP50kMv5p","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Play the cell to show a plot of training errors vs. epoch number\n","import csv\n","from matplotlib import pyplot as plt\n","\n","lossDataFromCSV = []\n","vallossDataFromCSV = []\n","\n","with open(QC_model_folder+'/Quality Control/training_evaluation.csv','r') as csvfile:\n","    csvRead = csv.reader(csvfile, delimiter=',')\n","    next(csvRead)\n","    for row in csvRead:\n","        lossDataFromCSV.append(float(row[0]))\n","        vallossDataFromCSV.append(float(row[1]))\n","\n","epochNumber = range(len(lossDataFromCSV))\n","plt.figure(figsize=(15,10))\n","\n","plt.subplot(2,1,1)\n","plt.plot(epochNumber,lossDataFromCSV, label='Training loss')\n","plt.plot(epochNumber,vallossDataFromCSV, label='Validation loss')\n","plt.title('Training loss and validation loss vs. epoch number (linear scale)')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch number')\n","plt.legend()\n","\n","plt.subplot(2,1,2)\n","plt.semilogy(epochNumber,lossDataFromCSV, label='Training loss')\n","plt.semilogy(epochNumber,vallossDataFromCSV, label='Validation loss')\n","plt.title('Training loss and validation loss vs. epoch number (log scale)')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch number')\n","plt.legend()\n","plt.savefig(os.path.dirname(QC_model_folder)+'/Quality Control/lossCurvePlots.png')\n","plt.show()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RZOPCVN0qcYb","colab_type":"text"},"source":["## **5.2. Error mapping and quality metrics estimation**\n","---\n","\n","<font size = 4>This section will display an overlay of the input images ground-truth (solid lines) and predicted boxes (dashed lines) as well as calculating the recall and precision of the predictions. The images provided in the \"Source_QC_folder\" and \"Target_QC_folder\" should contain images (e.g. as .jpg)and annotations (.xml files)!"]},{"cell_type":"code","metadata":{"id":"Nh8MlX3sqd_7","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Choose the folders that contain your Quality Control dataset\n","\n","Source_QC_folder = \"\" #@param{type:\"string\"}\n","Annotations_QC_folder = \"\" #@param{type:\"string\"}\n","\n","file_suffix = os.path.splitext(os.listdir(Source_QC_folder)[0])[1]\n","\n","# Create a quality control/Prediction Folder\n","if os.path.exists(QC_model_folder+\"/Quality Control/Prediction\"):\n","  shutil.rmtree(QC_model_folder+\"/Quality Control/Prediction\")\n","\n","os.makedirs(QC_model_folder+\"/Quality Control/Prediction\")\n","\n","#Delete old csv with box predictions if one exists\n","\n","if os.path.exists('/content/mycsv.csv'):\n","  os.remove('/content/mycsv.csv')\n","if os.path.exists(Source_QC_folder+'/.ipynb_checkpoints'):\n","  shutil.rmtree(Source_QC_folder+'/.ipynb_checkpoints')\n","\n","os.chdir('/content/gdrive/My Drive/keras-yolo2')\n","for img in os.listdir(Source_QC_folder):\n","  full_image_path = Source_QC_folder+'/'+img\n","  predict('config.json',QC_model_folder+'/best_weights.h5',full_image_path)\n","\n","\n","for img in os.listdir(Source_QC_folder):\n","  if img.endswith('detected'+file_suffix):\n","    shutil.move(Source_QC_folder+'/'+img,QC_model_folder+\"/Quality Control/Prediction/\"+img)\n","\n","### Get the coordinates of the predicted boxes, ###\n","### box classes and confidence scores           ###\n","\n","# from the csv containing the predicted boxes\n","with open('/content/mycsv.csv','r', newline='') as csvfile:\n","  csv_reader = csv.reader(csvfile)\n","  pred_boxes = []\n","  pred_classes = []\n","  pred_conf = []\n","  for row in csv_reader:\n","    image_boxes = []\n","    box_classes = []\n","    box_conf = []\n","    for i in range(0,len(row),6):\n","      image_boxes.append(list(map(float,row[i:i+4])))\n","      box_classes.append(int(row[i+5]))\n","      box_conf.append(float(row[i+4]))\n","    pred_boxes.append(image_boxes)   # The rows of this list contain the coordinates for all boxes per image\n","    pred_classes.append(box_classes) # The rows of this list contain the predicted classes for each box in the pred_boxes\n","    pred_conf.append(box_conf)       # The rows of this list contain the confidence scores for each predicted box in pred_boxes\n","\n","shutil.move('/content/mycsv.csv',QC_model_folder+\"/Quality Control/Prediction/predicted_boxes.csv\")\n","\n","#### Get the coordinates of the GT boxes ###\n","\n","df_anno_QC_gt = []\n","#dir_anno = Training_Source_annotations\n","for fnm in os.listdir(Annotations_QC_folder):  \n","    if not fnm.startswith('.'): ## do not include hidden folders/files\n","        tree = ET.parse(os.path.join(Annotations_QC_folder,fnm))\n","        row = extract_single_xml_file(tree)\n","        row[\"fileID\"] = os.path.splitext(fnm)[0]\n","        df_anno_QC_gt.append(row)\n","df_anno_QC_gt = pd.DataFrame(df_anno_QC_gt)\n","\n","maxNobj = np.max(df_anno_QC_gt[\"Nobj\"])\n","\n","config_path = '/content/gdrive/My Drive/keras-yolo2/config.json'\n","class_dict = {}\n","\n","with open(config_path) as config_buffer:\n","  config = json.load(config_buffer)\n","  for i in config[\"model\"][\"labels\"]:\n","    class_dict[i] = int(config[\"model\"][\"labels\"].index(i))\n","\n","reverse_class_dict = {value : key for (key, value) in class_dict.items()}\n","\n","df_anno_QC_gt = df_anno_QC_gt.replace(class_dict)\n","\n","gt_boxes = []\n","gt_labels = []\n","gt_label_names = []\n","for j in range(0,df_anno_QC_gt.shape[0]):\n","  row = df_anno_QC_gt.iloc[j]\n","  width = int(row[\"width\"])\n","  height = int(row[\"height\"])\n","  gt_box = []\n","  gt_label = []\n","  gt_label_name = []\n","  for i in range(row[\"Nobj\"]):\n","    label = int(float(row[\"bbx_{}_name\".format(i)]))\n","    label_name = row[\"bbx_{}_name\".format(i)]\n","    x1=row[\"bbx_{}_xmin\".format(i)]\n","    y1=row[\"bbx_{}_ymin\".format(i)]\n","    x2=row[\"bbx_{}_xmax\".format(i)]\n","    y2=row[\"bbx_{}_ymax\".format(i)]\n","    #gt_box.append([x1/width,y1/height,x2/width,y2/height])\n","    gt_box.append([x1,y1,x2,y2])\n","\n","    gt_label.append(label)\n","    gt_label_name.append(label_name)\n","  gt_boxes.append(gt_box)\n","  gt_labels.append(gt_label)\n","  gt_label_names.append(gt_label_name)\n","\n","#The essential outputs from this are gt_array and gt_classes_full\n","#Each row contains all bounding boxes and classes for each gt image.\n","\n","#Here we create the Detection Maps for the first three predictions\n","#Prediction\n","\n","pred_box_1 = np.array(pred_boxes[0])\n","pred_box_2 = np.array(pred_boxes[1])\n","#pred_box_3 = np.array(pred_boxes[2])\n","\n","pred_class_1 = np.array(pred_classes[0])\n","pred_class_2 = np.array(pred_classes[1])\n","#pred_class_3 = np.array(pred_classes[2])\n","\n","pred_conf_1 = np.array(pred_conf[0])\n","pred_conf_2 = np.array(pred_conf[1])\n","#pred_conf_3 = np.array(pred_conf[2])\n","                      \n","#print(pred_box_1)\n","\n","#print(pred_conf_1)\n","\n","# #GT\n","#print(gt_box_1[0])\n","gt_box_1 = np.array(gt_boxes[0])\n","gt_box_2 = np.array(gt_boxes[1])\n","#gt_box_3 = np.array(gt_boxes[2])\n","#print(gt_box_1)\n","\n","gt_class_1 = np.array(gt_labels[0])\n","gt_class_2 = np.array(gt_labels[1])\n","#gt_class_3 = np.array(gt_labels[2])\n","\n","frames = [(pred_box_2, pred_class_2, pred_conf_1, gt_box_1, gt_class_1)]#,\n","          #(pred_box_2, pred_class_2, pred_conf_2, gt_box_2, gt_class_2)]#,\n","          #(pred_box_3, pred_class_3, pred_conf_3, gt_box_3, gt_class_3)]\n","\n","n_class = 5\n","\n","mAP = DetectionMAP(n_class, overlap_threshold=0.3)\n","plt.figure(figsize=(15,5))\n","for i, frame in enumerate(frames):\n","  #print(i)\n","  img = np.array(io.imread(os.path.join(Source_QC_folder,os.path.splitext(os.listdir(Annotations_QC_folder)[i])[0]+file_suffix)))\n","  #print(os.listdir(Source_QC_folder)[i])\n","  #print(\"Evaluate frame {}\".format(i))\n","  show_frame(*frame, reverse_class_dict, background = img)\n","  \n","  mAP.evaluate(*frame)\n","\n","  mAP.plot()\n","\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-n9CLLJ77FAA","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Inspect example output from QC\n","import random\n","from matplotlib.pyplot import imread\n","# This will display a randomly chosen dataset input and predicted output\n","random_choice = random.choice(os.listdir(Source_QC_folder))\n","file_suffix = os.path.splitext(random_choice)[1]\n","\n","x = imread(Source_QC_folder+\"/\"+random_choice)\n","\n","#os.chdir(Result_folder)\n","y = imread(QC_model_folder+\"/Quality Control/Prediction/\"+os.path.splitext(random_choice)[0]+'_detected'+file_suffix)\n","#y = imread(os.path.dirname(QC_model_path)+\"/Quality Control/Prediction/\"+os.path.splitext(random_choice)[0]+file_suffix)\n","\n","\n","plt.figure(figsize=(30,15))\n","\n","plt.subplot(1,3,1)\n","plt.axis('off')\n","plt.imshow(x, interpolation='nearest')\n","plt.title('Input')\n","\n","plt.subplot(1,3,2)\n","plt.axis('off')\n","plt.imshow(y, interpolation='nearest')\n","plt.title('Predicted output');\n","\n","df_anno_QC_gt = []\n","#dir_anno = Training_Source_annotations\n","for fnm in os.listdir(Annotations_QC_folder):  \n","    if not fnm.startswith('.'): ## do not include hidden folders/files\n","        tree = ET.parse(os.path.join(Annotations_QC_folder,fnm))\n","        row = extract_single_xml_file(tree)\n","        row[\"fileID\"] = os.path.splitext(fnm)[0]\n","        df_anno_QC_gt.append(row)\n","df_anno_QC_gt = pd.DataFrame(df_anno_QC_gt)\n","\n","maxNobj = np.max(df_anno_QC_gt[\"Nobj\"])\n","\n","#Write the annotations to a csv file\n","\n","import imageio\n","for i in range(0,df_anno_QC_gt.shape[0]):\n","  #print(df_anno_QC_gt[i][\"fileID\"])\n","  if df_anno_QC_gt.iloc[i][\"fileID\"]+file_suffix == random_choice:\n","    row = df_anno_QC_gt.iloc[i]\n","\n","img  = imageio.imread(Source_QC_folder+'/'+random_choice)\n","      #row = df_anno_QC_gt.iloc[i,:]\n","    #plt.figure(figsize=(12,12))\n","plt.subplot(1,3,3)\n","plt.axis('off')\n","plt.imshow(img) # plot image\n","plt.title('Ground Truth annotations')\n","    # for each object in the image, plot the bounding box\n","for iplot in range(row[\"Nobj\"]):\n","    plt_rectangle(plt,\n","                  label = row[\"bbx_{}_name\".format(iplot)],\n","                  x1=row[\"bbx_{}_xmin\".format(iplot)],\n","                  y1=row[\"bbx_{}_ymin\".format(iplot)],\n","                  x2=row[\"bbx_{}_xmax\".format(iplot)],\n","                  y2=row[\"bbx_{}_ymax\".format(iplot)])#,\n","                  #fontsize=8)\n","plt.show() ## show the plot\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Esqnbew8uznk"},"source":["# **6. Using the trained model**\n","\n","---\n","\n","<font size = 4>In this section the unseen data is processed using the trained model (in section 4). First, your unseen images are uploaded and prepared for prediction. After that your trained model from section 4 is activated and finally saved into your Google Drive."]},{"cell_type":"markdown","metadata":{"id":"d8wuQGjoq6eN","colab_type":"text"},"source":["## **6.1. Generate prediction(s) from unseen dataset**\n","---\n","<\n","<font size = 4>The current trained model (from section 4.2) can now be used to process images. If you want to use an older model, untick the **Use_the_current_trained_model** box and enter the name and path of the model to use. Predicted output images are saved in your **Result_folder** folder as restored image stacks (ImageJ-compatible TIFF images).\n","\n","<font size = 4>**`Data_folder`:** This folder should contain the images that you want to use your trained network on for processing.\n","\n","<font size = 4>**`Result_folder`:** This folder will contain the predicted output images."]},{"cell_type":"code","metadata":{"id":"9ZmST3JRq-Ho","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ### Provide the path to your dataset and to the folder where the predictions are saved, then play the cell to predict outputs from your unseen images.\n","\n","Data_folder = \"\" #@param {type:\"string\"}\n","Result_folder = \"\" #@param {type:\"string\"}\n","file_suffix = os.path.splitext(os.listdir(Data_folder)[0])[1]\n","\n","# model name and path\n","#@markdown ###Do you want to use the current trained model?\n","Use_the_current_trained_model = True #@param {type:\"boolean\"}\n","\n","#@markdown ###If not, provide the name of the model and path to model folder:\n","\n","Prediction_model_path = \"\" #@param {type:\"string\"}\n","\n","#@markdown ###Which backend is the model using?\n","backend = \"Full Yolo\" #@param [\"Select Model\",\"Full Yolo\",\"Inception3\",\"SqueezeNet\",\"MobileNet\",\"Tiny Yolo\"]\n","os.chdir('/content/gdrive/My Drive/keras-yolo2')\n","if backend == \"Full Yolo\":\n","  if not os.path.exists('/content/gdrive/My Drive/keras-yolo2/full_yolo_backend.h5'):\n","    !wget https://github.com/rodrigo2019/keras_yolo2/releases/download/pre-trained-weights/full_yolo_backend.h5\n","elif backend == \"Inception3\":\n","  if not os.path.exists('/content/gdrive/My Drive/keras-yolo2/inception_backend.h5'):\n","    !wget https://github.com/rodrigo2019/keras_yolo2/releases/download/pre-trained-weights/inception_backend.h5\n","elif backend == \"MobileNet\":\n","  if not os.path.exists('/content/gdrive/My Drive/keras-yolo2/mobilenet_backend.h5'):\n","    !wget https://github.com/rodrigo2019/keras_yolo2/releases/download/pre-trained-weights/mobilenet_backend.h5\n","elif backend == \"SqueezeNet\":\n","  if not os.path.exists('/content/gdrive/My Drive/keras-yolo2/squeezenet_backend.h5'):\n","    !wget https://github.com/rodrigo2019/keras_yolo2/releases/download/pre-trained-weights/squeezenet_backend.h5\n","elif backend == \"Tiny Yolo\":\n","  if not os.path.exists('/content/gdrive/My Drive/keras-yolo2/tiny_yolo_backend.h5'):\n","    !wget https://github.com/rodrigo2019/keras_yolo2/releases/download/pre-trained-weights/tiny_yolo_backend.h5\n","if (Use_the_current_trained_model): \n","  print(\"Using current trained network\")\n","  Prediction_model_path = full_model_path\n","\n","if os.path.exists(Prediction_model_path+'/best_weights.h5'):\n","  print(\"The \"+os.path.basename(Prediction_model_path)+\" network will be used.\")\n","else:\n","  W  = '\\033[0m'  # white (normal)\n","  R  = '\\033[31m' # red\n","  print(R+'!! WARNING: The chosen model does not exist !!'+W)\n","  print('Please make sure you provide a valid model path and model name before proceeding further.')\n","\n","# Provide the code for performing predictions and saving them\n","print(\"Images saved into folder:\", Result_folder)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GcmBwMJVcFh1","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown ##Run Prediction\n","\n","#New_full_Prediction_model_path = \"\"\n","os.chdir('/content/gdrive/My Drive/keras-yolo2')\n","\n","if os.path.exists(Data_folder+'/.ipynb_checkpoints'):\n","  shutil.rmtree(Data_folder+'/.ipynb_checkpoints')\n","for img in os.listdir(Data_folder):\n","  full_image_path = Data_folder+'/'+img\n","  predict('config.json',Prediction_model_path+'/best_weights.h5',full_image_path)\n","\n","for img in os.listdir(Data_folder):\n","  if img.endswith('detected'+file_suffix):\n","    shutil.move(Data_folder+'/'+img,Result_folder+'/'+img)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EIe3CRD7XUxa","colab_type":"text"},"source":["## **6.2. Inspect the predicted output**\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"LmDP8xiwXTTL","colab_type":"code","cellView":"form","colab":{}},"source":["# @markdown ##Run this cell to display a randomly chosen input and its corresponding predicted output.\n","import random\n","from matplotlib.pyplot import imread\n","# This will display a randomly chosen dataset input and predicted output\n","random_choice = random.choice(os.listdir(Data_folder))\n","\n","x = imread(Data_folder+\"/\"+random_choice)\n","\n","os.chdir(Result_folder)\n","y = imread(Result_folder+\"/\"+os.path.splitext(random_choice)[0]+'_detected'+file_suffix)\n","\n","plt.figure(figsize=(16,8))\n","\n","plt.subplot(1,2,1)\n","plt.axis('off')\n","plt.imshow(x, interpolation='nearest')\n","plt.title('Input')\n","\n","plt.subplot(1,2,2)\n","plt.axis('off')\n","plt.imshow(y, interpolation='nearest')\n","plt.title('Predicted output');\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hvkd66PldsXB","colab_type":"text"},"source":["## **6.3. Download your predictions**\n","---\n","\n","<font size = 4>**Store your data** and ALL its results elsewhere by downloading it from Google Drive and after that clean the original folder tree (datasets, results, trained model etc.) if you plan to train or use new networks. Please note that the notebook will otherwise **OVERWRITE** all files which have the same name."]},{"cell_type":"markdown","metadata":{"id":"Rn9zpWpo0xNw","colab_type":"text"},"source":["\n","#**Thank you for using Yolo_v2!**"]}]}